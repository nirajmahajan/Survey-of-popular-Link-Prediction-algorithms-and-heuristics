# -*- coding: utf-8 -*-
"""Node2vec

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WOAZ3m4VlH7LJb5WGnwyyFr6PsB0KTU8
"""

import torch
import torch.nn as nn
from torch.nn import init
from torch.autograd import Variable
from torch.nn import init
import torch.nn.functional as F
from torch.autograd import Variable
import networkx as nx
import random
from random import sample, shuffle
import copy
import numpy as np 
import scipy as sp
import argparse as arg 
import pickle
import time
# from sklearn.metrics import f1_score
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from collections import defaultdict
np.random.seed(337)
# !pip3 install tqdm
# !pip3 install gensim
# !pip3 install joblib

# path = '/content/drive/My Drive/CS768-Project/node2vec/'
# import os
# os.chdir(path)
# !ls

def find_nbr_nonnbr(G):
	"""
	A routine that processes a networkx graph and emits list of neighbours and non-neighbours for each node.
	Input: NetworkX graph
	Returns: dictionary of neighbour and non-neighbors
	Do not use on large graphs since non-neighbour dictionary is O(n^3) storage, n: number of vertices. 
	"""

	vertex_set  = set(G.nodes())
	vertex_list = list(vertex_set)

	nbr_dict, nonnbr_dict = {}, {}

	for node in range(len(vertex_list)):
		nbr_set = set([nbr for nbr in G[node]])
		nonnbr_set = list(vertex_set - nbr_set)

		nbr_dict[node] = nbr_set
		nonnbr_dict[node] = nonnbr_set

	return nbr_dict, nonnbr_dict

def sigmoid(z):
	return 1/(1+np.exp(-1*z))

def normalize(x):
	x[:,1] = (x[:,1]-np.mean(x[:,1],axis=0))/np.std(x[:,1],axis=0)
	return x

class Graph:
	def __init__(self, filename):
		"""
		Initialize a NetworkX graph from a file with edge list.
		Raises Exception if provided file is not an edge list
		"""
		G = nx.read_edgelist(filename)
		self.G = nx.convert_node_labels_to_integers(G)
		self.vertex_set = set(self.G.nodes())
		self.vertex_list = list(self.vertex_set)
		l = []
		for (u,v) in list(G.edges):
			if u == v:
				l.append((u,v))
		for edge in l:
			G.remove_edge(edge)

	def split_train_test(self, test_fraction):
		"""
		Prepares the graph for training by creating a train, test graph with non-overlapping edges 
		Input test_fraction: Fraction of neighbours per node that make the test split.
		Returns: None
		Adds to the self test_edges_list, train_edges_list both of which are list of triples (in, out, edge-type)
		A new graph with edges from test omitted is attached to self called G_train. 
		"""
		assert test_fraction<=1 and test_fraction>=0

		self.test_fraction = test_fraction

		nbr_dict, nonnbr_dict = find_nbr_nonnbr(self.G)
		self.nbr_dict, self.nonnbr_dict = nbr_dict, nonnbr_dict

		per_node_train_set, per_node_test_set = {}, {}           
		test_edges_list, train_edges_list = [], []        
		for node in range(len(self.vertex_list)):            
			per_node_test_set[node], per_node_train_set[node] = {}, {}

			x_nbr = int(test_fraction*len(nbr_dict[node]))
			x_nonnbr = int(test_fraction*len(nonnbr_dict[node]))

			per_node_test_set[node]['nbr'] = sample(nbr_dict[node], x_nbr)
			per_node_train_set[node]['nbr'] =  list(set(nbr_dict[node])\
			                                       - set(per_node_test_set[node]['nbr']))

			per_node_test_set[node]['nonnbr'] = sample(nonnbr_dict[node], x_nonnbr)
			per_node_train_set[node]['nonnbr'] =  list(set(nonnbr_dict[node])\
			                                  - set(per_node_test_set[node]['nonnbr']))

			test_edges_per_node = [(node, x) for x in per_node_test_set[node]['nbr']]
			test_non_edges_per_node  = [(node, x) for x in per_node_test_set[node]['nonnbr']]
			train_edges_per_node = [(node, x) for x in per_node_train_set[node]['nbr']]
			train_non_edges_per_node  = [(node, x) for x in per_node_train_set[node]['nonnbr']]

			test_edges_list.extend([(a, b, 1) for a, b in test_edges_per_node if (a < b)])
			test_edges_list.extend([(a, b, 0) for a, b in test_non_edges_per_node if (a < b)])

			train_edges_list.extend([(a, b, 1) for a, b in train_edges_per_node if (a < b)])
			train_edges_list.extend([(a, b, 0) for a, b in train_non_edges_per_node if (a < b)])
            
		self.test_edges_list = test_edges_list         
		self.train_edges_list = train_edges_list

        # print(len(set(test_edges_list).intersection(set(train_edges_list))))

		G_train =  copy.deepcopy(self.G)
		G_train.remove_edges_from([(a, b) for (a, b, label) in test_edges_list if label==1])
		self.G_train = G_train

		G_test = nx.Graph()
		G_test.add_edges_from([(a, b) for (a, b, label) in test_edges_list if label==1])
		self.G_test = G_test

		G_test_inv = nx.Graph()
		G_test_inv.add_edges_from([(a, b) for (a, b, label) in test_edges_list if label==0])
		self.G_test_inv = G_test_inv

		# G_train_inv = nx.Graph()
		# G_train_inv.add_edges_from([(a, b) for (a, b, label) in train_edges_list if label==0])
		# self.G_train_inv = G_train_inv        
		print((G_train.number_of_edges()+G_test.number_of_edges())/self.G.number_of_edges())

	def eval(self,preds,node):
		preds = sorted(preds,key = lambda x:x[-1],reverse=True)
		cnt,precision,rr = 0,0,0
		found_first = False
		for i,(u,v,p) in enumerate(preds):
			# if i > 30:
			# 	break
			if ((u,v) in self.G_test.edges()) or ((v,u) in self.G_test.edges()):
				cnt += 1
				precision += cnt/(i+1)
				if not(found_first):
					rr = 1/(i+1)
					found_first = True
		avg_pre = precision/len(list(self.G_test[node]))
		return avg_pre,rr

graph = Graph('facebook.txt')
graph.split_train_test(.2)

from node2vec import Node2Vec

# FILES
EMBEDDING_FILENAME = './embeddings.emb'
EMBEDDING_MODEL_FILENAME = './embeddings.model'

# Create a graph
# graph = nx.fast_gnp_random_graph(n=100, p=0.5)

# Precompute probabilities and generate walks
node2vec = Node2Vec(graph.G_train, dimensions=64, walk_length=30, num_walks=200, workers=4)

## if d_graph is big enough to fit in the memory, pass temp_folder which has enough disk space
# Note: It will trigger "sharedmem" in Parallel, which will be slow on smaller graphs
#node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4, temp_folder="/mnt/tmp_data")

# Embed
model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `diemnsions` and `workers` are automatically passed (from the Node2Vec constructor)

# Look for most similar nodes
model.wv.most_similar('2')  # Output node names are always strings

# Save embeddings for later use
model.wv.save_word2vec_format(EMBEDDING_FILENAME)

# Save model for later use
model.save(EMBEDDING_MODEL_FILENAME)


def edge_feature(u, v):
	return u*v
	return abs(u-v)


train_list = graph.train_edges_list
train_edges = [(x[0],x[1]) for x in train_list if x[2] == 1]
train_non_edges = [(x[0],x[1]) for x in train_list if x[2] == 0]

m = 16
shuffle(train_edges)
train_edges = train_edges[:len(train_edges)//m]
shuffle(train_non_edges)
train_non_edges = train_non_edges[:len(train_non_edges)//m]

num_samples = len(train_edges)+len(train_non_edges)
dim = len(model.wv[str(graph.vertex_list[0])])
n = len(train_edges)
print(dim, n)

X_train = np.zeros((num_samples, dim))
Y_train = np.zeros(num_samples)
Y_train[:n] = 1

X_train[:n,:] = np.array([edge_feature(np.array(model.wv[str(x)]), np.array(model.wv[str(y)])) for x,y in train_edges])
X_train[n:,:] = np.array([edge_feature(np.array(model.wv[str(x)]), np.array(model.wv[str(y)])) for x,y in train_non_edges])

# clf = LogisticRegression().fit(X_train, Y_train)
# weights = clf.coef_

# weights = np.matmul(np.matmul(X_train.T, X_train), np.matmul(X_train.T, Y_train.reshape(-1,1))).reshape(-1)

svr = svm.SVR()
svr.fit(X_train, Y_train)


mean_ap,mrr=0,0
num_nodes = 0
for node in graph.vertex_list:

    if (node not in list(graph.G_test.nodes)) or (node not in list(graph.G_test_inv.nodes)):
        continue
    # print(node)

    #-------------------------------Get Pred-------------------------#
    lst = list(graph.G_test[node]) + list(graph.G_test_inv[node])
    # X_test = np.array([graphsage.sim(node,u).detach().numpy() for u in lst])
    # X_test = normalize(X_test)
    emb_node = np.array(model.wv[str(node)])
    # preds = np.array([np.dot(weights, edge_feature(emb_node, np.array(model.wv[str(u)]))) for u in lst])
    preds = np.array([svr.predict(edge_feature(emb_node, np.array(model.wv[str(u)]))) for u in lst])
    # preds = zip(lst,list(clf.predict_proba(X_test)[:,1]))
    preds = zip(lst,preds)
    preds = [(node,u,p) for u,p in preds]

    avg_pre, rr = graph.eval(preds,node)
    mean_ap += avg_pre
    mrr += rr
    num_nodes += 1
mean_ap /= num_nodes
mrr /= num_nodes

print("\nLogistic Regression")
print(f"MAP = {mean_ap}")
print(f"MRR = {mrr}")